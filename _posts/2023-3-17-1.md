---

title: ddl is April 7
categories:
- Damn
feature_image: "https://user-images.githubusercontent.com/53364734/192078882-190b1b14-a1ee-4590-ac1f-56ac81ffeb56.png"

---
Today topic: 实验室每天打卡
1. l2距离：L2距离又称欧几里得距离（Euclidean distance），是计算两个向量之间距离的一种常见方法。假设有两个n维向量a和b，它们的L2距离d为：

{% include latex.html latex="d = sqrt((a1 - b1)^2 + (a2 - b2)^2 + ... + (an - bn)^2)" %}

其中，ai和bi分别表示向量a和向量b在第i个维度上的取值。L2距离的计算方法就是将两个向量各个维度上的差值的平方相加后再开方。

L2距离在机器学习中经常用于计算两个样本之间的相似度或距离，特别是在聚类、降维、图像和语音处理等领域中。L2距离具有不依赖于坐标系的优点，因此被广泛应用于各种场景中。
2. Cross-entropy loss（交叉熵损失）是一种用于分类问题的损失函数，常用于神经网络的训练中。在分类问题中，每个样本被赋予一个标签，表示其属于哪个类别。模型通过对样本进行预测，得到每个类别的概率分布，交叉熵损失用于衡量模型的预测结果与真实标签之间的差距。

对于一个分类问题，假设有N个样本和K个类别，每个样本的真实标签用一个长度为K的向量y表示，其中第i个元素表示该样本是否属于第i个类别。模型的预测结果用一个长度为K的向量p表示，表示该样本属于每个类别的概率分布。则交叉熵损失为：

{% include latex.html latex="L = -1/N * sum(y * log(p) + (1 - y) * log(1 - p))" %}

其中，log表示自然对数，*表示向量的点乘运算，sum表示求和运算。可以看出，交叉熵损失将真实标签y和模型的预测结果p作为输入，通过求解两者之间的差距来更新模型参数，使得模型的预测结果更加接近真实标签。

交叉熵损失是一种广泛应用的损失函数，可以用于二分类、多分类、多标签分类等各种场景中。

<!-- more -->


更改链接：[![更改博客链接](https://user-images.githubusercontent.com/53364734/192180297-c1654533-eb5f-4bf9-aa9f-ab830208a5e3.png)](https://github.com/lizeyujack/lizeyujack.github.io/edit/main/_posts/2023-3-17-1.md)

- 使用leaf audio来训练deepship数据来提取模型的特征。生成图片。（任务大船/小船）。
- 对于图片使用小样本学习的策略来识别。孪生网络。

如何 证明mae不适合这个少样本分类网络?

- 跑一下mae的fine tune结果(复现层面)。
- 根据文章 A MUTUAL LEARNING FRAMEWORK FOR FEW-SHOT SOUND EVENT DETECTION，尝试搭建一个少样本分类网络。