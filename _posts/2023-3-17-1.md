---

title: ddl is April 7
categories:
- Damn
feature_image: "https://user-images.githubusercontent.com/53364734/192078882-190b1b14-a1ee-4590-ac1f-56ac81ffeb56.png"

---
Today topic: 实验室每天打卡
1. l2距离：L2距离又称欧几里得距离（Euclidean distance），是计算两个向量之间距离的一种常见方法。假设有两个n维向量a和b，它们的L2距离d为：

{% include latex.html latex="d = sqrt((a1 - b1)^2 + (a2 - b2)^2 + ... + (an - bn)^2)" %}

其中，ai和bi分别表示向量a和向量b在第i个维度上的取值。L2距离的计算方法就是将两个向量各个维度上的差值的平方相加后再开方。

L2距离在机器学习中经常用于计算两个样本之间的相似度或距离，特别是在聚类、降维、图像和语音处理等领域中。L2距离具有不依赖于坐标系的优点，因此被广泛应用于各种场景中。
2. Cross-entropy loss（交叉熵损失）是一种用于分类问题的损失函数，常用于神经网络的训练中。在分类问题中，每个样本被赋予一个标签，表示其属于哪个类别。模型通过对样本进行预测，得到每个类别的概率分布，交叉熵损失用于衡量模型的预测结果与真实标签之间的差距。

对于一个分类问题，假设有N个样本和K个类别，每个样本的真实标签用一个长度为K的向量y表示，其中第i个元素表示该样本是否属于第i个类别。模型的预测结果用一个长度为K的向量p表示，表示该样本属于每个类别的概率分布。则交叉熵损失为：

{% include latex.html latex="L = -1/N * sum(y * log(p) + (1 - y) * log(1 - p))" %}

其中，log表示自然对数，*表示向量的点乘运算，sum表示求和运算。可以看出，交叉熵损失将真实标签y和模型的预测结果p作为输入，通过求解两者之间的差距来更新模型参数，使得模型的预测结果更加接近真实标签。

交叉熵损失是一种广泛应用的损失函数，可以用于二分类、多分类、多标签分类等各种场景中。
3. 自监督学习（self-supervised learning）是指在无标注数据上进行的学习，通过模型自己生成标签或者利用数据本身的属性进行监督，从而学习到有用的特征表示。相比之下，有监督学习（supervised learning）是在有标注数据上进行的学习，通过输入数据和相应的标签对模型进行监督和训练，学习到输入数据和输出标签之间的映射关系。

自监督学习和有监督学习之间的关系可以从不同的角度进行理解。一方面，自监督学习可以被看作是有监督学习的一种特殊形式，因为自监督任务本质上也是在训练数据上产生标签或监督信息，并通过这些信息进行学习。另一方面，自监督学习也可以被看作是一种预训练方法，通过在大规模无标注数据上进行自监督训练，学习到通用的特征表示，然后将这些特征表示迁移到特定任务的有监督学习中进行微调和训练。

总的来说，自监督学习和有监督学习并不是对立的关系，而是可以相互补充和结合的两种学习方法。在实际应用中，根据数据的不同特点和任务的不同要求，可以灵活选择和组合不同的学习方法来提高模型的性能和泛化能力。

<!-- more -->


更改链接：[![更改博客链接](https://user-images.githubusercontent.com/53364734/192180297-c1654533-eb5f-4bf9-aa9f-ab830208a5e3.png)](https://github.com/lizeyujack/lizeyujack.github.io/edit/main/_posts/2023-3-17-1.md)

- 使用leaf audio来训练deepship数据来提取模型的特征。生成图片。（任务大船/小船）。
- 对于图片使用小样本学习的策略来识别。孪生网络。

如何 证明mae不适合这个少样本分类网络?

- 跑一下mae的fine tune结果(复现层面)。
- 根据文章 A MUTUAL LEARNING FRAMEWORK FOR FEW-SHOT SOUND EVENT DETECTION，尝试搭建一个少样本分类网络。