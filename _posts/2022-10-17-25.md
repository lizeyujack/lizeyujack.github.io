---

title: daiding
categories:
- 论文精读
feature_image: "https://user-images.githubusercontent.com/53364734/192078882-190b1b14-a1ee-4590-ac1f-56ac81ffeb56.png"

---

<!-- more -->


更改链接：[![更改博客链接](https://user-images.githubusercontent.com/53364734/192180297-c1654533-eb5f-4bf9-aa9f-ab830208a5e3.png)](https://github.com/lizeyujack/lizeyujack.github.io/edit/main/_posts/2022-10-17-25.md)
没有知识的
生成任务
prompt主要是分类任务中或者基于知识的任务比较适合。

# mark 一下
由于[SEP]标记的目的是充当两个句子之间的分隔符，因此它符合您使用[SEP]标记分隔查询和答案序列的目标。
您还可以尝试添加不同的标记来标记查询的开始和结束，或者将答案标记为<BOQ>和<EOQ>以标记查询的开始和结束。同样，<BOA>和<EOA>用于标记答案的开始和结束。
有时，使用现有的令牌比向词汇表添加新的令牌要好得多，因为它需要大量的训练迭代以及学习新的令牌嵌入的数据。
但是，如果您要添加新令牌(如果您的应用程序需要这样做)，则可以按如下方式添加：
```
{%raw%}
num_added_toks = tokenizer.add_tokens(['[EOT]'], special_tokens=True) ##This line is updated
model.resize_token_embeddings(len(tokenizer))

###The tokenizer has to be saved if it has to be reused
tokenizer.save_pretrained(<output_dir>)
{%endraw%}
```
很不错的[source](https://albertauyeung.github.io/2020/06/19/bert-tokenization.html/)学习如何将文字转换成向量。
  
---
分割线
  
# mark一下：
向[tokenizor](https://blog.csdn.net/icestorm_rain/article/details/108540053)中添加特殊token的写法很不错。
  
  
```
{%raw%}
for train_id in id[0]:
    new_list = []
    line_is = train_id[0]
    line = deque([line_is])
    dp_id = deque(train_id)
    dp_id.popleft()
    print(dp_id)
    while dp_id != deque([]):
        now_num = dp_id.popleft()
        if line_is==now_num:
            line.append(now_num)
        else:
            if len(list(line)) >= 2:# insert the list
                new_list.append(list(line))
                line = deque([now_num])
            elif len(list(line)) == 1:# just append to list
                new_list.append(line[0])
                line = deque([now_num])
            line_is = now_num
        if dp_id == deque([]):
            if len(list(line)) >= 2:# insert the list
                new_list.append(list(line))
            elif len(list(line)) == 1:# just append to list
                new_list.append(line[0])
{%endraw%}
```
