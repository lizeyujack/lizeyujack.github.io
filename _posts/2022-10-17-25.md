---

title: daiding
categories:
- 论文精读
feature_image: "https://user-images.githubusercontent.com/53364734/192078882-190b1b14-a1ee-4590-ac1f-56ac81ffeb56.png"

---

<!-- more -->


更改链接：[![更改博客链接](https://user-images.githubusercontent.com/53364734/192180297-c1654533-eb5f-4bf9-aa9f-ab830208a5e3.png)](https://github.com/lizeyujack/lizeyujack.github.io/edit/main/_posts/2022-10-17-25.md)
没有知识的
生成任务
prompt主要是分类任务中或者基于知识的任务比较适合。

# mark 一下
由于[SEP]标记的目的是充当两个句子之间的分隔符，因此它符合您使用[SEP]标记分隔查询和答案序列的目标。
您还可以尝试添加不同的标记来标记查询的开始和结束，或者将答案标记为<BOQ>和<EOQ>以标记查询的开始和结束。同样，<BOA>和<EOA>用于标记答案的开始和结束。
有时，使用现有的令牌比向词汇表添加新的令牌要好得多，因为它需要大量的训练迭代以及学习新的令牌嵌入的数据。
但是，如果您要添加新令牌(如果您的应用程序需要这样做)，则可以按如下方式添加：
```
{%raw%}
num_added_toks = tokenizer.add_tokens(['[EOT]'], special_tokens=True) ##This line is updated
model.resize_token_embeddings(len(tokenizer))

###The tokenizer has to be saved if it has to be reused
tokenizer.save_pretrained(<output_dir>)
{%endraw%}
```
很不错的[source](https://albertauyeung.github.io/2020/06/19/bert-tokenization.html/)学习如何将文字转换成向量。
  
---
分割线
  
# mark一下：
向[tokenizor](https://blog.csdn.net/icestorm_rain/article/details/108540053)中添加特殊token的写法很不错。
